{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa9b1232-fac4-4404-a269-91ae24838e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chakr\\AppData\\Local\\Temp\\ipykernel_21536\\549741780.py:17: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "C:\\Users\\chakr\\AppData\\Local\\Temp\\ipykernel_21536\\549741780.py:18: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "File path C:/Users/Nidhi.Chakravarthy/Documents/Financial-Document-parser-using-RAG/esg_regulations.pdf is not a valid file or url",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m     splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m splitter\u001b[38;5;241m.\u001b[39msplit_documents(pages)\n\u001b[1;32m---> 31\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m FAISS\u001b[38;5;241m.\u001b[39mfrom_documents(\u001b[43mload_esg_base\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, embeddings)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Analysis function for ESG compliance queries\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyze_content\u001b[39m(query, file):\n",
      "Cell \u001b[1;32mIn[2], line 26\u001b[0m, in \u001b[0;36mload_esg_base\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_esg_base\u001b[39m():\n\u001b[1;32m---> 26\u001b[0m     loader \u001b[38;5;241m=\u001b[39m \u001b[43mPyPDFLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/Nidhi.Chakravarthy/Documents/Financial-Document-parser-using-RAG/esg_regulations.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     pages \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m     28\u001b[0m     splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:281\u001b[0m, in \u001b[0;36mPyPDFLoader.__init__\u001b[1;34m(self, file_path, password, headers, extract_images, mode, images_parser, images_inner_format, pages_delimiter, extraction_mode, extraction_kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    240\u001b[0m     file_path: Union[\u001b[38;5;28mstr\u001b[39m, PurePath],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    250\u001b[0m     extraction_kwargs: Optional[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    251\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    252\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize with a file path.\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \n\u001b[0;32m    254\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;124;03m        `aload` methods to retrieve parsed documents with content and metadata.\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 281\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser \u001b[38;5;241m=\u001b[39m PyPDFParser(\n\u001b[0;32m    283\u001b[0m         password\u001b[38;5;241m=\u001b[39mpassword,\n\u001b[0;32m    284\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    290\u001b[0m         extraction_kwargs\u001b[38;5;241m=\u001b[39mextraction_kwargs,\n\u001b[0;32m    291\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:140\u001b[0m, in \u001b[0;36mBasePDFLoader.__init__\u001b[1;34m(self, file_path, headers)\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(temp_pdf)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path):\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile path \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not a valid file or url\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)\n",
      "\u001b[1;31mValueError\u001b[0m: File path C:/Users/Nidhi.Chakravarthy/Documents/Financial-Document-parser-using-RAG/esg_regulations.pdf is not a valid file or url"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import threading\n",
    "import schedule\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import gradio as gr\n",
    "\n",
    "# Initialize components\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "llm = Ollama(\n",
    "    model=\"llama2\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "# Load ESG base documents and initialize the base vector store\n",
    "def load_esg_base():\n",
    "    loader = PyPDFLoader(\"C:/Users/Nidhi.Chakravarthy/Documents/Financial-Document-parser-using-RAG/esg_regulations.pdf\")\n",
    "    pages = loader.load()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    return splitter.split_documents(pages)\n",
    "\n",
    "vector_store = FAISS.from_documents(load_esg_base(), embeddings)\n",
    "\n",
    "# Analysis function for ESG compliance queries\n",
    "def analyze_content(query, file):\n",
    "    try:\n",
    "        if file:\n",
    "            loader = PyPDFLoader(file.name)\n",
    "            user_docs = loader.load()\n",
    "            splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "            user_chunks = splitter.split_documents(user_docs)\n",
    "            user_store = FAISS.from_documents(user_chunks, embeddings)\n",
    "            # Merge user document vectors with base vector store\n",
    "            user_store.merge_from(vector_store)\n",
    "            retriever = user_store.as_retriever()\n",
    "        else:\n",
    "            retriever = vector_store.as_retriever()\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"Analyze this ESG document for compliance risks:\n",
    "{context}\n",
    "\n",
    "Query: {input}\n",
    "\n",
    "Format findings as:\n",
    "- [RISK LEVEL] [SECTION]: [DESCRIPTION]\"\"\"\n",
    "        )\n",
    "\n",
    "        chain = (\n",
    "            {\"context\": retriever, \"input\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "        )\n",
    "\n",
    "        return chain.invoke(query)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Global variable for tracking the last updated report timestamp\n",
    "last_updated_report = None\n",
    "\n",
    "# Function to fetch and update the regulatory report from an API\n",
    "def fetch_and_update_regulatory_report():\n",
    "    global last_updated_report, vector_store\n",
    "    # Replace with your actual regulatory reports endpoint and API key\n",
    "    api_url = \"https://api.regulations.gov/v4/documents?sort=-postedDate&api_key=DEMO_KEY\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(api_url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        documents = data.get(\"data\", [])\n",
    "        if not documents:\n",
    "            print(\"No regulatory reports found.\")\n",
    "            return\n",
    "\n",
    "        # Use the first (latest) report sorted by postedDate\n",
    "        latest_doc = documents[0]\n",
    "        updated_str = latest_doc.get(\"postedDate\")\n",
    "        if not updated_str:\n",
    "            print(\"No posted date found in latest report.\")\n",
    "            return\n",
    "\n",
    "        # Convert the posted date string to a datetime object (assumes ISO 8601 format)\n",
    "        updated_datetime = datetime.fromisoformat(updated_str.rstrip(\"Z\"))\n",
    "        \n",
    "        if last_updated_report is None or updated_datetime > last_updated_report:\n",
    "            last_updated_report = updated_datetime\n",
    "            print(f\"New regulatory report detected: {last_updated_report}\")\n",
    "\n",
    "            # Retrieve the PDF download URL from the metadata (adjust key as required)\n",
    "            doc_url = latest_doc.get(\"downloadUrl\")\n",
    "            if not doc_url:\n",
    "                print(\"Download URL not found for the latest report.\")\n",
    "                return\n",
    "\n",
    "            pdf_response = requests.get(doc_url)\n",
    "            if pdf_response.status_code != 200:\n",
    "                print(\"Failed to download the latest regulatory report.\")\n",
    "                return\n",
    "\n",
    "            # Save the PDF locally for processing\n",
    "            pdf_filename = \"latest_regulatory_report.pdf\"\n",
    "            with open(pdf_filename, \"wb\") as f:\n",
    "                f.write(pdf_response.content)\n",
    "\n",
    "            # Process the newly fetched report\n",
    "            loader = PyPDFLoader(pdf_filename)\n",
    "            pages = loader.load()\n",
    "            splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "            new_documents = splitter.split_documents(pages)\n",
    "\n",
    "            # Merge the new documents into the existing vector store\n",
    "            new_vector_store = FAISS.from_documents(new_documents, embeddings)\n",
    "            vector_store.merge_from(new_vector_store)\n",
    "            print(\"Vector store updated with the latest regulatory report.\")\n",
    "        else:\n",
    "            print(\"No new regulatory reports found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while fetching regulatory report: {e}\")\n",
    "\n",
    "# Polling function that schedules regular API checks for regulatory updates\n",
    "def poll_regulatory_reports():\n",
    "    fetch_and_update_regulatory_report()  # Run once immediately on startup\n",
    "    schedule.every(1).hours.do(fetch_and_update_regulatory_report)\n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "        time.sleep(60)\n",
    "\n",
    "# Gradio interface for the ESG analyzer\n",
    "with gr.Blocks(title=\"ESG Compliance Analyzer\") as app:\n",
    "    gr.Markdown(\"## ESG Document Analyzer with Local LLM\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            file_input = gr.File(label=\"Upload Document (PDF)\")\n",
    "            query_input = gr.Textbox(label=\"Your Compliance Question\")\n",
    "            submit_btn = gr.Button(\"Analyze\")\n",
    "        with gr.Column():\n",
    "            output = gr.Textbox(label=\"Analysis Results\", interactive=False)\n",
    "\n",
    "    submit_btn.click(\n",
    "        fn=analyze_content,\n",
    "        inputs=[query_input, file_input],\n",
    "        outputs=output\n",
    "    )\n",
    "\n",
    "# Main: start the regulatory polling thread and launch the Gradio UI\n",
    "if __name__ == \"__main__\":\n",
    "    # Start polling for regulatory updates in a background thread\n",
    "    polling_thread = threading.Thread(target=poll_regulatory_reports, daemon=True)\n",
    "    polling_thread.start()\n",
    "    \n",
    "    # Launch the Gradio app (this will block and open the UI in your browser)\n",
    "    app.launch(server_name=\"0.0.0.0\", share=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106f3025-74c6-4335-b9c1-7fa927249a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
